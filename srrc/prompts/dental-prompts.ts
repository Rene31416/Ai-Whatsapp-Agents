import { PromptTemplate } from "@langchain/core/prompts";
import { z } from "zod";
import { getLLM, CLINIC_CONTEXT } from "../services/llm.services";
import { JsonOutputParser } from "@langchain/core/output_parsers";

// ===== 1. What we return to the rest of the system =====
// We keep final_answer / identify_intent / confidence for backwards compat
// AND we surface the scheduling metadata so we can branch later.
export const DecisionLiteSchema = z.object({
  final_answer: z.string().max(400, "final_answer excede 400 chars"),

  identify_intent: z.boolean(), // <- maps from ii (did user give contact info?)
  confidence: z.number().min(0).max(1), // <- maps from c

  isCalendar: z.boolean(),
  // readyToSchedule: z.boolean(),

  // appt: z.object({
  //   procedure: z.string().min(1).max(100).nullable(),
  //   needsDoctorReview: z.boolean().nullable(),
  //   patientName: z.string().min(1).max(120).nullable(),
  //   phone: z.string().min(1).max(40).nullable(),
  //   apptAt: z.string().min(1).max(80).nullable(), // ISO8601 UTC like "2025-11-03T21:00:00Z" or null
  //   notes: z.string().min(1).max(200).nullable(),
  // }),
});
export type DecisionLite = z.infer<typeof DecisionLiteSchema>;

// ===== 2. Raw shape we expect FROM the LLM =====
// This is EXACTLY what the model must output each turn.
const CompactSchema = z.object({
  a: z.string().max(400), // WhatsApp answer
  ii: z.boolean(), // did user give/update THEIR contact info this turn?
  c: z.number().min(0).max(1), // confidence in ii
  isCalendar: z.boolean(),

  // appt: z.object({
  //   procedure: z.string().min(1).max(100).nullable(),
  //   needsDoctorReview: z.boolean().nullable(),
  //   patientName: z.string().min(1).max(120).nullable(),
  //   phone: z.string().min(1).max(40).nullable(),
  //   apptAt: z.string().min(1).max(80).nullable(), // UTC timestamp string if user gave a clear date+hora
  //   notes: z.string().min(1).max(200).nullable(),
  // }),

  // readyToSchedule: z.boolean(), // true ONLY if:
  // intent === "schedule",
  // appt.needsDoctorReview === false,
  // appt.patientName, appt.phone, appt.apptAt are all non-null
});
type Compact = z.infer<typeof CompactSchema>;

function extractAnyText(msg: any): string {
  if (!msg) return "";
  if (typeof msg?.content === "string") return msg.content;
  if (Array.isArray(msg?.content)) {
    const t = msg.content
      .map((p: any) => p?.text || p?.content || "")
      .filter(Boolean)
      .join("\n");
    if (t.trim()) return t;
  }
  const gen = msg?.generations?.[0]?.[0];
  const gText = gen?.text ?? gen?.message?.content;
  if (typeof gText === "string" && gText.trim()) return gText;
  const parts =
    msg?.response?.candidates?.[0]?.content?.parts ??
    msg?.candidates?.[0]?.content?.parts;
  if (Array.isArray(parts)) {
    const t = parts
      .map((p: any) => p?.text || "")
      .filter(Boolean)
      .join("\n");
    if (t.trim()) return t;
  }
  try {
    return JSON.stringify(msg);
  } catch {
    return String(msg ?? "");
  }
}

function getFinishReason(msg: any): string | undefined {
  return (
    msg?.additional_kwargs?.finishReason ??
    msg?.response_metadata?.finishReason ??
    msg?.kwargs?.additional_kwargs?.finishReason
  );
}

function stripJsonFences(s: string): string {
  const fenced = s.trim().match(/^```(?:json)?\s*([\s\S]*?)\s*```$/i);
  if (fenced) return fenced[1].trim();
  return s.trim();
}

function rescueJsonSlice(s: string): string | null {
  const first = s.indexOf("{");
  const last = s.lastIndexOf("}");
  if (first >= 0 && last > first) return s.slice(first, last + 1);
  return null;
}

function ms(from: bigint, to: bigint) {
  return Number(to - from) / 1e6;
}

export async function decideAndAnswerLite(input: {
  message: string;
  facts_header: string;
  recent_window: string;
  now_iso: string;
  now_human: string;
  tz: string;
}): Promise<DecisionLite> {
  const base = await getLLM();
  const clinic = CLINIC_CONTEXT;

  const clinic_compact = [
    clinic.name,
    clinic.address,
    clinic.hours,
    clinic.phone,
    clinic.website,
  ]
    .filter(Boolean)
    .join(" | ");

  const parser = new JsonOutputParser<Compact>();

  // model tuning
  const tuned =
    (base as any).bind?.({
      temperature: 0.25,
      top_p: 0.9,
      maxOutputTokens: 400,
      responseMimeType: "application/json",
      safetySettings: [
        { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_NONE" },
        { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_NONE" },
        {
          category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
          threshold: "BLOCK_NONE",
        },
        {
          category: "HARM_CATEGORY_DANGEROUS_CONTENT",
          threshold: "BLOCK_NONE",
        },
      ],
    }) ?? base;

  // =========================================
  // PROMPT TEMPLATE (SYSTEM INSTRUCTIONS)
  // =========================================

  // ---------- MAIN (ROUTER) PROMPT ----------
const prompt = new PromptTemplate({
  inputVariables: [
    "message",
    "facts_header",
    "recent_window",
    "clinic_compact",
    "now_iso",
    "now_human",
    "tz",
  ],
  template: `
Responde SIEMPRE en espaÃ±ol, estilo WhatsApp, con mÃ¡ximo 1â€“2 emojis. SonÃ¡ natural, cero callcenter.

âš ï¸ INSTRUCCIÃ“N CRÃTICA:
- Tu respuesta DEBE ser un Ãºnico objeto JSON con las claves indicadas mÃ¡s abajo. Si agregÃ¡s texto fuera del JSON, la respuesta se descarta.

SALUDO / PRESENTACIÃ“N:
- GREET_OK=true indica que esta es la primera interacciÃ³n real (o pasaron >8h).
- GREET_OK=false significa que ya te presentaste antes; no repitas saludo aunque VENTANA muestre saludos previos.
- Solo saluda y presÃ©ntate si GREET_OK=true Y el MENSAJE ACTUAL contiene un saludo (â€œholaâ€, â€œbuenos dÃ­asâ€, â€œquÃ© talâ€, etc.).
- Si GREET_OK=false, tu respuesta NO debe contener frases como â€œHolaâ€, â€œBuenasâ€, â€œSoy el asistenteâ€¦â€, ni ninguna presentaciÃ³n nueva; empieza directo con el contenido Ãºtil.
- Respuestas que violen lo anterior (ej. â€œÂ¡Hola! Soyâ€¦â€) se consideran INCORRECTAS.
- Ejemplo GREET_OK=true: usa un saludo breve y natural variando el fraseo (p.ej. â€œÂ¡Hola! Soy el asistente de Opal Dental ðŸ˜Š Â¿CÃ³mo te ayudo hoy?â€, â€œÂ¡Buenas! Te escribe el asistente virtual de Opal Dental ðŸ˜Š Â¿En quÃ© te apoyo?â€, â€œHey, soy tu asistente en Opal Dental ðŸ˜Š Â¿CÃ³mo puedo ayudarte?â€).
- Ejemplo GREET_OK=false (consulta directa): Usuario: â€œQuiero saber la ubicaciÃ³n exacta de la clÃ­nica por favorâ€ â†’ Respuesta: â€œEstamos en 123 Main St, San Salvador. Â¿NecesitÃ¡s algo mÃ¡s?â€
  - Ejemplo INCORRECTO con GREET_OK=false: Usuario: â€œOtra cosa, Â¿sabÃ©s agendar citas?â€ â†’ ðŸš« â€œÂ¡Hola! Soy el asistenteâ€¦â€ (no lo repitas).
  - Ejemplo CORRECTO con GREET_OK=false: Usuario: â€œOtra cosa, Â¿sabÃ©s agendar citas?â€ â†’ â€œClaro, te ayudo a coordinar tu cita. DÃ©jame verificar quÃ© necesitÃ¡s.â€

COMPORTAMIENTO GENERAL:
- Si el primer mensaje trae saludo + intenciÃ³n, priorizÃ¡ la intenciÃ³n. Si es agenda, ruteÃ¡ (ver abajo) y no respondas localmente.
- Si el usuario se estÃ¡ despidiendo (p. ej., â€œgracias, eso serÃ­a todoâ€, â€œno por ahoraâ€, â€œadiÃ³sâ€, â€œhasta luegoâ€), cerrÃ¡ amable SIN â€œÂ¿algo mÃ¡s?â€.
- Despedidas: confirma que no falta nada y usa una sola frase amable sin ofrecer ayuda adicional. VariÃ¡ el fraseo (ej.: â€œTodo listo, cualquier cosa me avisÃ¡s ðŸ˜Šâ€, â€œPerfecto, quedo atento ðŸ˜Šâ€) para evitar respuestas calcadas consecutivas.
- â€œGraciasâ€ aislado NO es despedida: podÃ©s ofrecer ayuda suave.

ROL DE ESTE NODO (Customer Service â€“ informaciÃ³n general):
- Aclarar/resumir lo que el usuario pide en MSG.
- Responder SOLO informaciÃ³n general de la CLÃNICA si el MSG lo pide explÃ­citamente (direcciÃ³n, horarios, telÃ©fono).
- Detectar si el MENSAJE ACTUAL trae/actualiza datos personales del usuario (ii).
- Este nodo NO maneja reglas de agenda ni recolecta datos para citas.

RUTEO A AGENDA (criterio por contexto):
- SeteÃ¡ "isCalendar": true y dejÃ¡ "a": "" (cadena vacÃ­a) cuando ocurra CUALQUIERA de estas condiciones:
  1) El MSG sugiere/insinÃºa acciones de citas (agendar, reagendar, cancelar, confirmar, consultar disponibilidad).
  2) El MSG APORTA o CORRIGE alguno de los datos mÃ­nimos de cita: nombre completo, nÃºmero de contacto, correo electrÃ³nico o doctor preferido.
  3) Considerando VENTANA + MSG, se continÃºa claramente un flujo de agenda (p. ej., el turno previo pidiÃ³ esos datos).
- Ejemplo de ruteo obligatorio: Usuario: â€œOtra cosa, Â¿sabÃ©s agendar citas?â€ â†’ isCalendar=true y a="".


LÃMITES (generales, sin lÃ³gica de agenda):
- No inventes procesos internos ni acceso a sistemas.
- No des informaciÃ³n que no figure en CLÃNICA.
- No pidas datos personales salvo que el usuario los ofrezca espontÃ¡neamente (si los da, marcÃ¡ ii=true).

MICROCOPY (tono breve y Ãºtil):
- Agradecimientos del usuario: respuesta corta + oferta suave (â€œÂ¡Con gusto! ðŸ˜Š Â¿Algo mÃ¡s en que te ayudo?â€).
- Si el usuario solo comparte identidad (ii=true) sin pedir agenda: agradecÃ© y dejÃ¡ puerta abierta (â€œÂ¡Gracias! Lo tengo anotado ðŸ˜Š Â¿En quÃ© te ayudo?â€).
- EvitÃ¡ monosÃ­labos secos (â€œokâ€, â€œlistoâ€) salvo cierre explÃ­cito.

PRIORIDAD ENTRE MARCAS:
- Si en el mismo turno detectÃ¡s ii=true (datos personales) y tambiÃ©n se cumple ruteo de agenda, entonces:
  - isCalendar=true
  - a=""
  - (ii puede quedar en true o false; la orquestaciÃ³n prioriza el ruteo)

VENTANA (orden y alcance):
- VENTANA contiene los Ãºltimos 10 mensajes ANTERIORES al MSG, del mÃ¡s viejo al mÃ¡s reciente (oldest â†’ newest).
- VENTANA NO incluye MSG. UsÃ¡ principalmente MSG, y VENTANA solo como apoyo.

MENSAJE "a" (polÃ­tica de salida):
- Si "isCalendar" = true â†’ "a" debe ser "" (vacÃ­o), porque la respuesta la darÃ¡ el agente de calendario.
- Si "isCalendar" = false â†’ "a" debe ser una respuesta breve (mÃ¡x 2 frases / 400 caracteres), respetando GREET_OK y usando CLÃNICA solo si el MSG lo pidiÃ³.

SALIDA ESTRICTA (solo UN JSON vÃ¡lido, sin texto extra ni backticks):
- Devuelve EXACTAMENTE un objeto JSON con estas claves (sustituÃ­ los valores segÃºn corresponda):
  {{"a":"...","c":0.7,"isCalendar":false,"ii":false}}
- No incluyas texto fuera del JSON ni mÃºltiples objetos.

CONTEXTO DISPONIBLE:
CLÃNICA: {clinic_compact}
FACTS: {facts_header}
VENTANA: {recent_window}
MSG: {message}
TIEMPO: {now_iso} | {now_human} ({tz})
`.trim(),
});


  console.info(
    `[decide][in] msg_len=${(input.message || "").length} facts_len=${
      (input.facts_header || "").length
    } recent_len=${(input.recent_window || "").length}`
  );

  // Expandimos la "ventana" para que el modelo recuerde la cita en curso.
  // (antes ~600 chars, ahora ~1200 aprox, p/ ~15 msgs recientes)
  const t0 = process.hrtime.bigint();
  const rendered = await prompt.format({
    message: (input.message ?? "").slice(0, 400),
    facts_header: (input.facts_header ?? "").slice(0, 200),
    recent_window: (input.recent_window ?? "").slice(0, 1200),
    clinic_compact: clinic_compact.slice(0, 240),
    now_iso: input.now_iso,
    now_human: input.now_human,
    tz: input.tz,
  });

  // debug: FACTS line
  const factsLineMatch = rendered.match(/FACTS:\s*([\s\S]*?)\nVENTANA:/);
  const factsRendered = factsLineMatch?.[1] ?? "(facts not found)";
  console.info(
    `[llm.input/FACTS.line]: "${factsRendered
      .slice(0, 160)
      .replace(/\n/g, "\\n")}"`
  );
  const flagMatch = factsRendered.match(/\[GREET_OK=(true|false)\]/i);
  console.info("[llm.input/GREET_OK.detected]:", flagMatch?.[1] ?? "(missing)");

  const tRender = process.hrtime.bigint();

  // Logs de entrada
  const vis = (s: string, n = 240) => s.replace(/\s+/g, " ").trim().slice(0, n);
  const countLines = (s: string) => (s ? s.split(/\r?\n/).length : 0);

  console.info(
    `[llm.input] chars=${rendered.length} preview="${rendered
      .slice(0, 300)
      .replace(/\n/g, "\\n")}${rendered.length > 300 ? "â€¦" : ""}"`
  );
  console.info(
    `[llm.input/MSG] len=${(input.message ?? "").length} lines=${countLines(
      input.message
    )} vis="${vis(input.message)}"`
  );
  console.info(
    `[llm.input/FACTS] len=${(input.facts_header ?? "").length} vis="${vis(
      input.facts_header
    )}"`
  );
  console.info(
    `[llm.input/VENTANA] len=${
      (input.recent_window ?? "").length
    } lines=${countLines(input.recent_window)} vis="${vis(
      input.recent_window,
      320
    )}"`
  );
  console.log("//////////////////////// VENATABA ////////////////");
  console.info(`[llm.input/VENTANA] ${input.recent_window}`);

  // Invoke LLM
  const tInvokeStart = process.hrtime.bigint();
  const llmOut: any = await tuned.invoke(rendered);
  const tInvokeEnd = process.hrtime.bigint();

  // Timings / usage
  console.info(
    `[llm.timing] render_ms=${ms(t0, tRender).toFixed(1)} invoke_ms=${ms(
      tInvokeStart,
      tInvokeEnd
    ).toFixed(1)} total_ms=${ms(t0, tInvokeEnd).toFixed(1)}`
  );
  const usage =
    llmOut?.usage_metadata ?? llmOut?.response_metadata?.tokenUsage ?? {};
  const promptTok =
    usage.promptTokens ?? usage.input_tokens ?? usage.inputTokens;
  const completionTok =
    usage.completionTokens ?? usage.output_tokens ?? usage.completionTokens;
  const totalTok =
    usage.totalTokens ??
    usage.total_tokens ??
    (promptTok ?? 0) + (completionTok ?? 0);
  if (promptTok || completionTok || totalTok) {
    console.info(
      `[llm.usage] prompt=${promptTok ?? "?"} completion=${
        completionTok ?? "?"
      } total=${totalTok ?? "?"}`
    );
  }

  // Raw output
  const fin = getFinishReason(llmOut);
  const rawText = extractAnyText(llmOut) ?? "";
  const cleanedText = stripJsonFences(rawText);
  console.info(
    `[llm.output] finish=${fin ?? "?"} chars=${
      cleanedText.length
    } preview="${cleanedText.slice(0, 300).replace(/\n/g, "\\n")}${
      cleanedText.length > 300 ? "â€¦" : ""
    }"`
  );

  if ((!cleanedText || !cleanedText.trim()) && fin && fin !== "STOP") {
    throw new Error(
      `MODEL_FINISH(${fin}): Sin contenido. Usage=${JSON.stringify(usage)}`
    );
  }

  // Parse + validate
  const tParseStart = process.hrtime.bigint();
  let compact: Compact;
  try {
    compact = await parser.parse(cleanedText);
  } catch (err: any) {
    const rescued = rescueJsonSlice(cleanedText);
    if (rescued) {
      try {
        compact = await parser.parse(rescued);
        console.warn("[decide][rescue] Gemini devolviÃ³ texto + JSON; se extrajo el objeto vÃ¡lido.");
      } catch (rescErr: any) {
        const msg = (rescErr?.message || String(rescErr)).slice(0, 500);
        throw new Error(
          `PARSE_ERROR(decideAndAnswerLite): JSON invÃ¡lido tras rescue. Detalle: ${msg}. Raw="${cleanedText.slice(
            0,
            400
          )}"`
        );
      }
    } else {
      const fallbackText = cleanedText.trim();
      if (fallbackText) {
        console.warn(
          "[decide][fallback.trigger] Gemini no devolviÃ³ JSON puro; se pedirÃ¡ aclaraciÃ³n al usuario."
        );
        compact = {
          a: "DisculpÃ¡, no entendÃ­ bien. Â¿PodÃ©s repetir o clarificar tu mensaje? ðŸ˜Š",
          c: 0.5,
          isCalendar: false,
          ii: false,
        };
      } else {
        const msg = (err?.message || String(err)).slice(0, 500);
        throw new Error(
          `PARSE_ERROR(decideAndAnswerLite): JSON invÃ¡lido. Detalle: ${msg}. Raw="${cleanedText.slice(
            0,
            400
          )}"`
        );
      }
    }
  }
  const tParseEnd = process.hrtime.bigint();

  const ok = CompactSchema.safeParse(compact);
  if (!ok.success) {
    const issues = ok.error.issues
      ?.map((i) => `${i.path.join(".")}: ${i.message}`)
      .join("; ");
    throw new Error(
      `VALIDATION_ERROR(decideAndAnswerLite): Claves/Tipos invÃ¡lidos. Issues: ${issues}. Raw="${rawText.slice(
        0,
        400
      )}"`
    );
  }
  const tValidateEnd = process.hrtime.bigint();

  console.info(
    `[llm.breakdown] render=${ms(t0, tRender).toFixed(1)}ms invoke=${ms(
      tInvokeStart,
      tInvokeEnd
    ).toFixed(1)}ms parse=${ms(tParseStart, tParseEnd).toFixed(
      1
    )}ms validate=${ms(tParseEnd, tValidateEnd).toFixed(1)}ms total=${ms(
      t0,
      tValidateEnd
    ).toFixed(1)}ms`
  );

  // Map to DecisionLite
  const mapped: DecisionLite = {
    final_answer: ok.data.a,
    identify_intent: ok.data.ii,
    confidence: ok.data.c,
    isCalendar: ok.data.isCalendar,
    // readyToSchedule: ok.data.readyToSchedule,
    // appt: {
    //   procedure: ok.data.appt.procedure,
    //   needsDoctorReview: ok.data.appt.needsDoctorReview,
    //   patientName: ok.data.appt.patientName,
    //   phone: ok.data.appt.phone,
    //   apptAt: ok.data.appt.apptAt,
    //   notes: ok.data.appt.notes,
    // },
  };

  const finOk = DecisionLiteSchema.safeParse(mapped);
  if (!finOk.success) {
    const issues = finOk.error.issues
      ?.map((i) => `${i.path.join(".")}: ${i.message}`)
      .join("; ");
    throw new Error(`VALIDATION_ERROR(decideAndAnswerLite.mapped): ${issues}.`);
  }

  const out = finOk.data;
  console.info(
    `[decide][out] a_len=${out.final_answer.length} ii=${
      out.identify_intent
    } c=${out.confidence.toFixed(2)} isCalendar=${out.isCalendar}`
  );

  return out;
}
